{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fd1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# === Load Dataset ===\n",
    "df = pd.read_csv(\"deduplicated_filtered_output.csv\")  # Replace with actual filename\n",
    "\n",
    "# === Deduplicate based on tx_id and token_balance_change ===\n",
    "df = df.drop_duplicates(subset=[\"tx_id\", \"token_balance_change\"])\n",
    "\n",
    "# === Standardize column names ===\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# === Rename and parse columns ===\n",
    "df.rename(columns={\n",
    "    \"address\": \"wallet\",\n",
    "    \"token_mint_address\": \"token\",\n",
    "    \"token_balance_change\": \"amount\",\n",
    "    \"block_time\": \"timestamp\",\n",
    "    \"price_usd\": \"usd_price\"\n",
    "}, inplace=True)\n",
    "\n",
    "# === Parse datetime ===\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "# === Remove extreme transactions (> 1 billion tokens moved) ===\n",
    "df = df[df[\"amount\"].abs() <= 1e9]\n",
    "\n",
    "\n",
    "# === Token name mapping ===\n",
    "TOKEN_NAMES = {\n",
    "    \"FUAfBo2jgks6gB4Z4LfZkqSZgzNucisEHqnNebaRxM1P\": \"MELANIA\",\n",
    "    \"6p6xgHyF7AeE6TZkSmFsko444wqoP15icUSqi2jfGiPN\": \"TRUMP\",\n",
    "    \"Bo9jh3wsmcC2AjakLWzNmKJ3SgtZmXEcSaW7L2FAvUsU\": \"LIBRA\"\n",
    "}\n",
    "\n",
    "# === Settings ===\n",
    "interval = \"H\"\n",
    "output_dir = \"token_analysis_outputs2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Process each token ===\n",
    "for token, token_df in df.groupby(\"token\"):\n",
    "    token_df = token_df.sort_index()\n",
    "    token_label = TOKEN_NAMES.get(token, token[:8])\n",
    "\n",
    "    # === USD Volume Calculation ===\n",
    "    token_df[\"usd_volume\"] = token_df[\"amount\"].abs() * token_df[\"usd_price\"]\n",
    "\n",
    "    # === Metric calculations ===\n",
    "    usd_volume = token_df[\"usd_volume\"].resample(interval).sum().rename(\"token_volume_usd\")\n",
    "    raw_volume = token_df[\"amount\"].abs().resample(interval).sum().rename(\"token_volume_tokens\")\n",
    "\n",
    "    unique_wallets = token_df.groupby(pd.Grouper(freq=interval))[\"wallet\"].nunique().rename(\"unique_wallets\")\n",
    "    tx_count = token_df.resample(interval)[\"wallet\"].count().rename(\"transaction_count\")\n",
    "\n",
    "    token_df[\"is_new_wallet\"] = ~token_df[\"wallet\"].duplicated()\n",
    "    new_wallets = token_df[token_df[\"is_new_wallet\"]].resample(interval)[\"wallet\"].count().rename(\"new_wallets\")\n",
    "\n",
    "    # Sniper detection\n",
    "    first_tx_time = token_df.index.min()\n",
    "    sniper_cutoff = first_tx_time + pd.Timedelta(minutes=3)\n",
    "    token_df[\"is_sniper\"] = token_df.index <= sniper_cutoff\n",
    "    sniper_activity = token_df[token_df[\"is_sniper\"]].resample(interval)[\"wallet\"].count().rename(\"sniper_tx_count\")\n",
    "\n",
    "    # Wash trading detection\n",
    "    wash_traders = set()\n",
    "    for wallet, group in token_df.groupby(\"wallet\"):\n",
    "        group = group.sort_index()\n",
    "        if len(group) < 6:\n",
    "            continue\n",
    "        group[\"direction\"] = group[\"amount\"].apply(lambda x: \"buy\" if x > 0 else \"sell\")\n",
    "        group[\"alt_trade\"] = group[\"direction\"] != group[\"direction\"].shift()\n",
    "        group[\"rolling_alt_count\"] = group[\"alt_trade\"].rolling(\"5min\").sum()\n",
    "        group[\"rolling_usd_volume\"] = (group[\"amount\"].abs() * group[\"usd_price\"]).rolling(\"5min\").sum()\n",
    "        if ((group[\"rolling_alt_count\"] >= 6) & (group[\"rolling_usd_volume\"] >= 100)).any():\n",
    "            wash_traders.add(wallet)\n",
    "\n",
    "    token_df[\"is_wash_trader\"] = token_df[\"wallet\"].isin(wash_traders)\n",
    "    wash_trading_activity = token_df[token_df[\"is_wash_trader\"]].resample(interval)[\"wallet\"].count().rename(\"wash_tx_count\")\n",
    "\n",
    "    avg_tx_size = token_df[\"amount\"].abs().resample(interval).mean().rename(\"avg_tx_size\")\n",
    "\n",
    "    # === Combine metrics ===\n",
    "    summary = pd.concat([\n",
    "        usd_volume,\n",
    "        raw_volume,\n",
    "        tx_count,\n",
    "        unique_wallets,\n",
    "        new_wallets,\n",
    "        sniper_activity,\n",
    "        wash_trading_activity,\n",
    "        avg_tx_size\n",
    "    ], axis=1).fillna(0)\n",
    "\n",
    "    # Ensure all columns are float type for clean CSV output\n",
    "    summary = summary.astype(float)\n",
    "\n",
    "    # === Save full CSV with clean numeric formatting ===\n",
    "    summary.to_csv(\n",
    "        f\"{output_dir}/metrics_{token_label}.csv\",\n",
    "        float_format=\"%.5f\"\n",
    "    )\n",
    "\n",
    "    # === Clip outliers for plotting (99.5%) ===\n",
    "    summary_clipped = summary.copy()\n",
    "    for col in summary_clipped.columns:\n",
    "        upper = summary_clipped[col].quantile(0.995)\n",
    "        summary_clipped[col] = summary_clipped[col].clip(upper=upper)\n",
    "\n",
    "    # === Save individual plots for each metric ===\n",
    "    for column in summary_clipped.columns:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        summary_clipped[column].plot(title=column.replace('_', ' ').title())\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/{token_label}_{column}.png\")\n",
    "        plt.close()\n",
    "\n",
    "print(\"âœ… Done. Metrics and plots saved to:\", output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
